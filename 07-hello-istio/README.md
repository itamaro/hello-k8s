# Hello Istio

Ref: https://github.com/thesandlord/Istio101

```sh
PROJECT_ID=$(gcloud config list project --format=flattened | awk 'FNR == 1 {print $2}')
GCLOUD_USER=$(gcloud config get-value core/account)

# set up RBAC
kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$GCLOUD_USER
# deploy Istio
make deploy-istio
# build and deploy test app
docker build -t gcr.io/$PROJECT_ID/istiotest:1.0 code/
docker push gcr.io/$PROJECT_ID/istiotest:1.0
make deploy-stuff
make get-stuff
kubectl get svc -w
# wait for frontend service external IP to resolve
export LB_IP="$( kubectl get svc frontend -o jsonpath='{.status.loadBalancer.ingress[0].ip}' )"
curl http://$LB_IP/
```

Access the app public IP (Ingress), observe the app behavior across refreshes.

Note the 404 - by default, Istio does not allow egress traffic (accessing services that are external to the cluster).

In a second terminal session, start monitoring services (command holds open kubectl port-forward - shouldn't return):

```sh
make start-monitoring-services
```

To allow the application egress to the external service:

```sh
./istio-0.6/bin/istioctl create -f configs/istio/egress.yaml
watch -n1 "curl -s http://$LB_IP/"
```

Now the app is able to access the external service - no more 404.

Now notice the flip-flop between prod and canary - the service "catches" both, and load balances traffic across all pods with uniform distribution.

This can also be observed on the traffic-flow graph, auto-generated by Istio - open "Web Preview" on port 8088 and access `/dotviz`.

To force all traffic to go through prod:

```sh
./istio-0.6/bin/istioctl create -f configs/istio/routing-1.yaml
watch -n1 "curl -s http://$LB_IP/"
```

Now let's use the app "backdoor" header to trigger failures with 50% probability:

```sh
watch -n1 "curl -s -i -H 'fail: 0.5' http://$LB_IP/"
```

We can try to increase robustness by asking Istio to retry failed requests automatically up to 3 times (note - no change in the application!):

```sh
./istio-0.6/bin/istioctl delete -f configs/istio/routing-1.yaml
./istio-0.6/bin/istioctl create -f configs/istio/routing-2.yaml
watch -n1 "curl -s -i -H 'fail: 0.5' http://$LB_IP/"
```

This is still flaky, because the Ingress->Frontend link is not managed by Istio.

We can delete the cloud based LB and use Istio's LB instead:

```sh
kubectl delete svc frontend
kubectl apply -f configs/kube/services-2.yaml
# get new IP
make get-stuff
kubectl get ing -w
# wait for external IP of the Ingress to resolve
export LB_IP="$( kubectl get ing istio-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}' )"
watch -n1 "curl -s -i -H 'fail: 0.3' http://$LB_IP/"
```

Let's add a way for developers to go through the canary using a "secret" header:

```sh
./istio-0.6/bin/istioctl create -f configs/istio/routing-3.yaml
watch -n1 "curl -s -H 'x-dev-user: super-secret' http://$LB_IP/"
watch -n1 "curl -s -H 'x-dev-user: nope' http://$LB_IP/"
```

Take a look at Grafana (open Web Preview on port 3000 and access `/dashboard/db/istio-dashboard`).

Cleanup:

```sh
kubectl delete service frontend
kubectl delete ingress istio-ingress
```
